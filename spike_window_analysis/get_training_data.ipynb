{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts information about time and location of spikes from the hdf5 file with the BrainGrid simulation data and converts it into a dataframe with the following columns: time_step (time step when the spike happened), xloc, yloc (coordinates of the spike). It then extracts spacial and temporal pre-burst and non-burst windows, labels them, and saves the result in a csv.\n",
    "\n",
    "Author: Mariia Lundvall (lundvm@uw.edu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gc\n",
    "import numpy.ma as ma\n",
    "import boto3\n",
    "import io\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename, bucket='braingrid-data'):\n",
    "    \"\"\"\n",
    "    Loads a csv file from s3. \n",
    "    \n",
    "    Args:\n",
    "        filename(str): the path (key) to the csv file.\n",
    "        bucket(str): the name of the s3 bucket to load the data from. Default is 'braingrid-data'. \n",
    "        \n",
    "    Returns:\n",
    "        df(pandas df): loaded csv as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    s3 = boto3.client('s3') \n",
    "    obj = s3.get_object(Bucket= bucket, Key= filename) \n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_s3(df, bucket='braingrid-data', key):\n",
    "    \"\"\"\n",
    "    Saves pandas dataframe to s3 as a gz file.\n",
    "    \n",
    "    Args:\n",
    "        df(pandas dataframe): dataframe to save to s3\n",
    "        bucket(str): the name of the bucket ('braingrid-data' by default).\n",
    "        key(str): the file key (the path where to save the file + the file name)\n",
    "    \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    \n",
    "    # create s3 client\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    # write DF to string stream\n",
    "    print('creating csv buffer')\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    # reset stream position\n",
    "    csv_buffer.seek(0)\n",
    "    # create binary stream\n",
    "    gz_buffer = io.BytesIO()\n",
    "\n",
    "    # compress string stream using gzip\n",
    "    print('compressing')\n",
    "    with gzip.GzipFile(mode='w', fileobj=gz_buffer) as gz_file:\n",
    "        gz_file.write(bytes(csv_buffer.getvalue(), 'utf-8'))\n",
    "        \n",
    "    # write stream to S3\n",
    "    print('writing to ss')\n",
    "    obj = client.put_object(Bucket=bucket, Key=key, Body=gz_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spike_matrix(f):\n",
    "    \"\"\"\n",
    "    Takes in an HDF5 file produced by the BrainGrid simuation, extracts information about time \n",
    "    and location of spikes and converts it into a dataframe with the following columns: time_step \n",
    "    (time step when the spike happened), xloc, yloc (coordinates of the spike).\n",
    "    \n",
    "    Args: \n",
    "        f(HDF5): loaded HDF5 file to read data from.\n",
    "    \n",
    "    Returns:\n",
    "        spikes_loc(pandas dataframe): dataframe that contains spiking data. The dataframe has the \n",
    "            following columns: time_step (time step when the spike happened), xloc, yloc \n",
    "            (coordinates of the spike).\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the spikes time steps and coordinates data from the hdf5 file and convert the data \n",
    "    # into numpy arrays\n",
    "    spikes = np.array(f['/spikesProbedNeurons'], dtype='uint32')\n",
    "    xloc = np.array(f['/xloc'], dtype='uint8')\n",
    "    yloc = np.array(f['/yloc'], dtype='uint8')\n",
    "    # m is the max number of spikes per neuron, n is the number of neurons in the simulation\n",
    "    m, n = spikes.shape\n",
    "    # transform the spikes matrix:\n",
    "    # 1. Traspose so that each row is a sequence of spikes of one neuron (instead of a column)\n",
    "    # 2. Flattem the matrix \n",
    "    # 3. Reshape into a 2d array from (m*n, ) to (m*n, 1). This is needed for further processing. \n",
    "    spikes = np.transpose(spikes).flatten().reshape(m*n, 1)\n",
    "    # create a mask to remove non-spikes (where time step=0)\n",
    "    mask = ma.masked_equal(spikes, 0).reshape(m*n, )\n",
    "    # Transform the coordinate vectors:\n",
    "    # 1. Make the vectors match the time step sequence. Repeat the values so that first m values \n",
    "    # in the x and y vectors are the coordinates of the first neuron\n",
    "    # 2. Remove the coordinates corresponding to non-spikes\n",
    "    # 3. Concatenate x and y, the result is an array xy of shape (m*n, 2)\n",
    "    xloc = np.compress(mask, np.repeat(xloc, m).reshape(m*n, 1), axis=0)\n",
    "    yloc = np.compress(mask, np.repeat(yloc, m).reshape(m*n, 1), axis=0)\n",
    "    xy = np.concatenate((xloc, yloc), axis=1)\n",
    "    # delete xloc and yloc to free memory\n",
    "    del xloc\n",
    "    del yloc\n",
    "    gc.collect()\n",
    "    # Remove non-spikes from the time step array, concatenate it to xy, \n",
    "    # and convert the result into a dataframe\n",
    "    spikes_loc = pd.DataFrame(np.concatenate((np.compress(mask, spikes, axis=0), xy), axis=1))\n",
    "    # delete spikes to free the memory\n",
    "    del spikes\n",
    "    gc.collect()\n",
    "    spikes_loc.rename(columns={0:'time_step', 1:'xloc', 2:'yloc'}, inplace=True)\n",
    "    spikes_loc.sort_values(by='time_step', inplace=True)\n",
    "    spikes_loc.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return spikes_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(windows, gap, spikes, x, y, mask, n):\n",
    "    \"\"\"\n",
    "    Takes in the spiking data, extracts spacial and temporal pre-bursts and non-bursts \n",
    "    windows, adds a label (0 for non-burst, 1 for pre-burst) to each window, and saves \n",
    "    the windows as a csv (one csv per window size, separate csvs with temporal and spacial data).\n",
    "\n",
    "    Args:\n",
    "        windows(list of ints): the list containing window sizes\n",
    "        gap(int): the size of the gap between the beginning of a burst and a non-burst window.\n",
    "        spikes(numpy 1d array): array contaning spikes time steps\n",
    "        x(numpy 1d array): array contaning x coordinates of the spikes\n",
    "        y(numpy 1d array): array containing y coordinates of the spikes\n",
    "        mask(boolean array): array of the same length as spikes, containing True for time steps \n",
    "            that indicate the starts of bursts\n",
    "        n(int): number of identified bursts in the simulation\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "        saves csv files with temporal and spacial training data\n",
    "    \"\"\"\n",
    "    m = spikes.shape[0]\n",
    "    for w in tqdm(windows):\n",
    "        print('Extracting windows of size ' + str(w))\n",
    "        # initialize pre-burst and non-burst arrays\n",
    "        temp_preburst = np.ones((n, w+1))\n",
    "        temp_nonburst = np.zeros((n, w+1))\n",
    "        space_preburst = np.ones((n, w*2+1))\n",
    "        space_nonburst = np.zeros((n, w*2+1))\n",
    "        index = 0\n",
    "        for i in range(1, m):\n",
    "            if not mask[i-1] and mask[i]:\n",
    "                # attempt to remove bad bursts. If the gap width is smaller than the\n",
    "                # gap size, we must be catching the beginning of a burst, so don't extract those windows\n",
    "                if spikes[i] - spikes[i-gap] >= gap:\n",
    "                    # get the temporal windows\n",
    "                    temp_preburst[index, 0:w] = spikes[i-w:i] - spikes[i-w]\n",
    "                    temp_nonburst[index, 0:w] = spikes[i -\n",
    "                                                       w-gap:i-gap] - spikes[i-w-gap]\n",
    "                    # get the spacial windows\n",
    "                    space_preburst[index, 0:w*2] = np.concatenate(\n",
    "                        (x[i-w:i].reshape(w, 1), y[i-w:i].reshape(w, 1)), axis=1).flatten()\n",
    "                    space_nonburst[index, 0:w*2] = np.concatenate((x[i-w-gap:i-gap].reshape(\n",
    "                        w, 1), y[i-w-gap:i-gap].reshape(w, 1)), axis=1).flatten()\n",
    "                    index += 1\n",
    "        if index < n:\n",
    "            temp_preburst = np.delete(temp_preburst, np.s_[index:n], 0)\n",
    "            temp_nonburst = np.delete(temp_nonburst, np.s_[index:n], 0)\n",
    "            space_preburst = np.delete(space_preburst, np.s_[index:n], 0)\n",
    "            space_nonburst = np.delete(space_nonburst, np.s_[index+1:n], 0)\n",
    "\n",
    "        # combine and shuffle data\n",
    "        temp_traindata = np.vstack((temp_preburst, temp_nonburst)).astype(int)\n",
    "        np.random.shuffle(temp_traindata)\n",
    "        space_traindata = np.vstack(\n",
    "            (space_preburst, space_nonburst)).astype(int)\n",
    "        np.random.shuffle(space_traindata)\n",
    "        # save the result as a csv\n",
    "        np.savetxt('train_data/temp_' + str(w) + '.csv',\n",
    "                   temp_traindata, delimiter=',')\n",
    "        np.savetxt('train_data/space_' + str(w) + '.csv',\n",
    "                   space_traindata, delimiter=',')\n",
    "        print('Total bursts: ' + str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tsim', 'burstinessHist', 'neuronThresh', 'neuronTypes', 'probedNeurons', 'radiiHistory', 'ratesHistory', 'simulationEndTime', 'spikesHistory', 'spikesProbedNeurons', 'starterNeurons', 'xloc', 'yloc'] \n",
      "\n",
      "[<HDF5 dataset \"Tsim\": shape (1,), type \"<f4\">, <HDF5 dataset \"burstinessHist\": shape (60000,), type \"<i4\">, <HDF5 dataset \"neuronThresh\": shape (10000,), type \"<f4\">, <HDF5 dataset \"neuronTypes\": shape (10000,), type \"<i4\">, <HDF5 dataset \"probedNeurons\": shape (10000,), type \"<i4\">, <HDF5 dataset \"radiiHistory\": shape (601, 10000), type \"<f4\">, <HDF5 dataset \"ratesHistory\": shape (601, 10000), type \"<f4\">, <HDF5 dataset \"simulationEndTime\": shape (1,), type \"<f4\">, <HDF5 dataset \"spikesHistory\": shape (6000000,), type \"<i4\">, <HDF5 dataset \"spikesProbedNeurons\": shape (375898, 10000), type \"<u8\">, <HDF5 dataset \"starterNeurons\": shape (1000,), type \"<i4\">, <HDF5 dataset \"xloc\": shape (10000,), type \"<i4\">, <HDF5 dataset \"yloc\": shape (10000,), type \"<i4\">]\n"
     ]
    }
   ],
   "source": [
    "#load an hdf5 file with the simulation data\n",
    "f = h5py.File('tR_1.0--fE_0.90.h5', 'r')\n",
    "print(list(f.keys()), '\\n')\n",
    "print(list(f.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 39s, sys: 3min 25s, total: 11min 4s\n",
      "Wall time: 10min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create the spiking data matrix\n",
    "spikes_loc = create_spike_matrix(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# save the result to s3 (optional)\n",
    "pandas_to_s3(spikes_loc, 'spikes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430471c2e9734318986acb828873be22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting windows of size 5\n",
      "Total bursts: 9710\n",
      "Extracting windows of size 10\n",
      "Total bursts: 9710\n",
      "Extracting windows of size 50\n",
      "Total bursts: 9710\n",
      "Extracting windows of size 100\n",
      "Total bursts: 9710\n",
      "Extracting windows of size 500\n",
      "Total bursts: 9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in burst data\n",
    "bursts = read_csv('allBurst.csv')['StartT'].values\n",
    "# initialize variables\n",
    "windows = [5, 10, 50, 100, 500]\n",
    "gap = 2000\n",
    "mask = np.in1d(spikes_loc.values[:, 0], bursts)\n",
    "n = bursts.shape[0]\n",
    "# get the training data\n",
    "get_training_data(\n",
    "    windows, gap, spikes_loc.values[:, 0], spikes_loc.values[:, 1], spikes_loc.values[:, 2], mask, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
